{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import truncnorm\n",
    "from scipy.optimize import check_grad\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    if ('train_images' not in globals()):  \n",
    "        # In ipython, use \"run -i homework2_template.py\" to avoid re-loading of data\n",
    "        train_images = np.load(\"mnist_train_images.npy\")\n",
    "        train_labels = np.load(\"mnist_train_labels.npy\")\n",
    "        validation_images = np.load(\"mnist_validation_images.npy\")\n",
    "        validation_labels = np.load(\"mnist_validation_labels.npy\")\n",
    "        test_images = np.load(\"mnist_test_images.npy\")\n",
    "        test_labels = np.load(\"mnist_test_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(row):\n",
    "    \"\"\"softmax function\n",
    "    read in one line predicted result(np.array) return softmaxed predicted result(np.array)\n",
    "    \"\"\"\n",
    "    row_sum = np.sum(np.exp(row), axis=1, keepdims=True)\n",
    "    return np.array(1.0 * np.exp(row) / row_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(z1):\n",
    "    \"\"\"relu input np.array retuen np.array\"\"\"\n",
    "    return np.maximum(z1, 0, z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def J(true_labels, y_hat):\n",
    "    \"\"\"true_labels and y_hat both in (1, 10) shape, true_labels should after one-hot\"\"\"\n",
    "    return - np.mean(np.sum(np.array(true_labels) * np.log(np.array(y_hat) + 1e-5), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        \"\"\"hidden_layers\n",
    "        \n",
    "        Args : \n",
    "            params : {\n",
    "                \"hidden_layers\" : [30, 40, 50], \n",
    "                \"lr\" : [0.001, 0.005, 0.01, 0.05, 0.1, 0.5],\n",
    "                \"batch_size\" : [16, 32, 64, 128, 256],\n",
    "                \"epochs\" : not this time\n",
    "                \"reg\" : not this time}\n",
    "        \"\"\"\n",
    "        self.fir = 784\n",
    "        self.h1 = params[\"hidden_layers\"]\n",
    "        self.h2 = 10\n",
    "            \n",
    "        self.w1 = truncnorm(-1.0/np.sqrt(self.fir), 1.0/np.sqrt(self.fir), loc = 0, scale = 1).\\\n",
    "            rvs(self.fir * self.h1).reshape(self.fir, self.h1)\n",
    "        self.b1 = np.expand_dims(np.ones(self.h1), axis = 0)\n",
    "        \n",
    "#         self.w1 = 0.01 * np.random.randn(self.fir, self.h1)\n",
    "#         self.b1 = np.zeros((1, self.h1))\n",
    "        \n",
    "#         self.w2 = truncnorm(-1.0/np.sqrt(self.h1),1.0/np.sqrt(self.h1), loc = 0, scale = 1).\\\n",
    "#             rvs(self.h1 * self.h2).reshape(self.h1, self.h2)\n",
    "#         self.b2 = np.expand_dims(np.ones(self.h2), axis = 0)\n",
    "        \n",
    "        self.w2 = 0.01 * np.random.randn(self.h1,self.h2)\n",
    "        self.b2 = np.zeros((1,self.h2))\n",
    "\n",
    "        self.lr = params[\"lr\"]\n",
    "        self.batch_size = params[\"batch_size\"]\n",
    "        self.epoches = 20\n",
    "        self.reg = 0\n",
    "    \n",
    "    def feed_forward(self, single_train):\n",
    "        \n",
    "        z1 = single_train.dot(self.w1) + self.b1\n",
    "        h1 = relu(z1)\n",
    "        z2 = h1.dot(self.w2) + self.b2\n",
    "        y_hat = softmax(z2)\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    def sgd(self, training, validation = None):\n",
    "        \"\"\"train with mini-batch stochastic gradient descent\n",
    "        \n",
    "        Args:\n",
    "            training : (training_features, training_labels)\n",
    "            validation : if validation is not None, caculate score for validation data\n",
    "        \"\"\"        \n",
    "        n_train = len(training)\n",
    "        for epoch in xrange(self.epoches):\n",
    "            random.shuffle(training)\n",
    "            mini_batches = [\n",
    "                training[k: k + self.batch_size] for k in xrange(0, n_train, self.batch_size)]\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch)\n",
    "        \n",
    "            if validation != None :\n",
    "                cost, accu = self.evaluate(validation)\n",
    "                print \"Epoch {0} : cost : {1}, accuracy : {2}\".format(epoch, cost, accu)\n",
    "            \n",
    "    def update_mini_batch(self, mini_batch):\n",
    "        \"\"\"update the gradient descent\"\"\"\n",
    "        \n",
    "        origin_list = [self.w1, self.b1, self.w2, self.b2]\n",
    "        nabla_list = [np.zeros_like(ele) for ele in origin_list]\n",
    "\n",
    "        for x, y in mini_batch:\n",
    "            dw1, db1, dw2, db2 = self.back_prop(x, y)\n",
    "            for i, d_ele in enumerate([dw1, db1, dw2, db2]):\n",
    "                nabla_list[i] += d_ele\n",
    "\n",
    "        self.w1 -= (self.lr) * nabla_list[0]\n",
    "        self.b1 -= (self.lr) * nabla_list[1]\n",
    "        self.w2 -= (self.lr) * nabla_list[2]\n",
    "        self.b2 -= (self.lr) * nabla_list[3]\n",
    "    \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"returns dw1, db1, dw2, db2\"\"\"\n",
    "        \n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        y = np.expand_dims(y, axis=0)\n",
    "        \n",
    "        # feed forward process, store info in each layer\n",
    "        z1 = x.dot(self.w1) + self.b1\n",
    "        h1 = relu(z1)\n",
    "        z2 = h1.dot(self.w2) + self.b2\n",
    "        y_hat = softmax(z2)\n",
    "        \n",
    "        # gradient descent for w2\n",
    "        dj_dw2 = (h1.T).dot(y_hat - y)\n",
    "\n",
    "        # gradient descent for b2\n",
    "        dj_db2 = (y_hat - y)\n",
    "\n",
    "        # gradient descent for w1\n",
    "        dj_dh1 = (y_hat - y).dot((self.w2).T)\n",
    "        dj_dz1 = dj_dh1\n",
    "        dj_dz1[h1 <= 0] = 0\n",
    "        dj_dw1 = (x.T).dot(dj_dz1)\n",
    "\n",
    "        # gradient descent for b1\n",
    "        dj_db1 = dj_dz1\n",
    "        \n",
    "        return dj_dw1, dj_db1, dj_dw2, dj_db2\n",
    "\n",
    "    def evaluate(self, validation_data):\n",
    "        \"\"\"return cost function and accruacy of test data\"\"\"\n",
    "        x, y = zip(*validation_data)\n",
    "        x = np.array(x) \n",
    "        f = np.array(y)\n",
    "        \n",
    "        pred_y = self.feed_forward(x)\n",
    "        validation_cost = J(y, pred_y)\n",
    "        \n",
    "        validation_result = [(np.argmax(self.feed_forward(x)), np.argmax(y)) for (x, y) in validation_data]\n",
    "        validation_accuracy = 1.0 * sum([int(x == y) for (x, y) in validation_result]) / len(validation_data)\n",
    "        \n",
    "        return validation_cost, validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : cost : 0.308377287118, accuracy : 0.9136\n",
      "Epoch 1 : cost : 0.245541219914, accuracy : 0.9328\n",
      "Epoch 2 : cost : 0.214253852991, accuracy : 0.938\n",
      "Epoch 3 : cost : 0.18853502858, accuracy : 0.948\n",
      "Epoch 4 : cost : 0.174031442265, accuracy : 0.9524\n",
      "Epoch 5 : cost : 0.165388689087, accuracy : 0.9536\n",
      "Epoch 6 : cost : 0.15024389401, accuracy : 0.9592\n",
      "Epoch 7 : cost : 0.144458984468, accuracy : 0.9582\n",
      "Epoch 8 : cost : 0.140685968856, accuracy : 0.9612\n",
      "Epoch 9 : cost : 0.133892238529, accuracy : 0.9608\n",
      "Epoch 10 : cost : 0.131011087433, accuracy : 0.9622\n",
      "Epoch 11 : cost : 0.125808519769, accuracy : 0.9632\n",
      "Epoch 12 : cost : 0.126024901887, accuracy : 0.963\n",
      "Epoch 13 : cost : 0.120963819866, accuracy : 0.965\n",
      "Epoch 14 : cost : 0.119806590523, accuracy : 0.9646\n",
      "Epoch 15 : cost : 0.116638950322, accuracy : 0.9674\n",
      "Epoch 16 : cost : 0.116894112478, accuracy : 0.9668\n",
      "Epoch 17 : cost : 0.115245916975, accuracy : 0.9676\n",
      "Epoch 18 : cost : 0.112089990017, accuracy : 0.9684\n",
      "Epoch 19 : cost : 0.117996194567, accuracy : 0.9646\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"hidden_layers\" : 30, \n",
    "    \"lr\" : 1e-3,\n",
    "    \"batch_size\" : 64}\n",
    "\n",
    "network = Network(params)\n",
    "\n",
    "network.sgd(zip(train_images, train_labels), zip(validation_images, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_best_hyperparameters(params_set, training, validation):\n",
    "    \n",
    "    params_names = sorted(params_set)\n",
    "    combinations = [dict(zip(params_names, prod)) for prod in it.product(*(params_set[params_name] for params_name in params_names))]\n",
    "    \n",
    "    best_cost = 999\n",
    "    best_param = {}\n",
    "    \n",
    "    for param in combinations:\n",
    "        network = Network(param)\n",
    "        network.sgd(training)\n",
    "        validation_cost, _ = network.evaluate(validation)\n",
    "        print param, validation_cost\n",
    "        if validation_cost < best_cost:\n",
    "            best_cost, best_param = validation_cost, param \n",
    "    \n",
    "    return best_cost, best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.01, 'hidden_layers': 30, 'batch_size': 16} 0.155601163059\n",
      "{'lr': 0.05, 'hidden_layers': 30, 'batch_size': 16} 0.428617656988\n",
      "{'lr': 0.01, 'hidden_layers': 40, 'batch_size': 16} 0.129053777354\n",
      "{'lr': 0.05, 'hidden_layers': 40, 'batch_size': 16} 0.280194917033\n",
      "{'lr': 0.01, 'hidden_layers': 30, 'batch_size': 32} 0.152826319933\n",
      "{'lr': 0.05, 'hidden_layers': 30, 'batch_size': 32} 2.31428256441\n",
      "{'lr': 0.01, 'hidden_layers': 40, 'batch_size': 32} 0.144604216982\n",
      "{'lr': 0.05, 'hidden_layers': 40, 'batch_size': 32} 2.10697256953\n",
      "{'lr': 0.01, 'hidden_layers': 30, 'batch_size': 64} 0.139669146717\n",
      "{'lr': 0.05, 'hidden_layers': 30, 'batch_size': 64} 2.31112862687\n",
      "{'lr': 0.01, 'hidden_layers': 40, 'batch_size': 64} 0.143870090566\n",
      "{'lr': 0.05, 'hidden_layers': 40, 'batch_size': 64} 2.30988781137\n"
     ]
    }
   ],
   "source": [
    "params_set = {\"hidden_layers\" : [30, 40], \"lr\" : [ 0.01, 0.05], \"batch_size\" : [16, 32, 64]}\n",
    "best_cost, best_param = find_best_hyperparameters(params_set, zip(train_images, train_labels), zip(validation_images, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.01, 'hidden_layers': 40, 'batch_size': 16}\n"
     ]
    }
   ],
   "source": [
    "print best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : cost : 0.160174910338, accuracy : 0.9512\n",
      "Epoch 1 : cost : 0.133626592051, accuracy : 0.9591\n",
      "Epoch 2 : cost : 0.125676406288, accuracy : 0.963\n",
      "Epoch 3 : cost : 0.118558162746, accuracy : 0.9652\n",
      "Epoch 4 : cost : 0.113817139577, accuracy : 0.965\n",
      "Epoch 5 : cost : 0.116495357458, accuracy : 0.9672\n",
      "Epoch 6 : cost : 0.115578345192, accuracy : 0.9664\n",
      "Epoch 7 : cost : 0.113126698501, accuracy : 0.9699\n",
      "Epoch 8 : cost : 0.116774003229, accuracy : 0.968\n",
      "Epoch 9 : cost : 0.10468873998, accuracy : 0.9703\n",
      "Epoch 10 : cost : 0.116532868574, accuracy : 0.969\n",
      "Epoch 11 : cost : 0.112363022409, accuracy : 0.9701\n",
      "Epoch 12 : cost : 0.140069874108, accuracy : 0.9646\n",
      "Epoch 13 : cost : 0.131174958875, accuracy : 0.967\n",
      "Epoch 14 : cost : 0.133099556212, accuracy : 0.9672\n",
      "Epoch 15 : cost : 0.145560494738, accuracy : 0.9622\n",
      "Epoch 16 : cost : 0.139369965082, accuracy : 0.9666\n",
      "Epoch 17 : cost : 0.135183335411, accuracy : 0.9693\n",
      "Epoch 18 : cost : 0.137486369113, accuracy : 0.9676\n",
      "Epoch 19 : cost : 0.137693581797, accuracy : 0.9688\n"
     ]
    }
   ],
   "source": [
    "network_test = Network(best_param)\n",
    "network_test.sgd(zip(train_images, train_labels), zip(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the check_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = train_images[:1]\n",
    "y = train_labels[:1]\n",
    "\n",
    "network = Network(params)\n",
    "w1, b1, w2 ,b2 = network.w1, network.b1, network.w2, network.b2\n",
    "\n",
    "shape_list = []\n",
    "for ele in [w1, b1, w2, b2]:\n",
    "    shape_list.append(ele.shape)\n",
    "\n",
    "def pack_wb(w1, b1, w2 ,b2) :\n",
    "    packed_value = []\n",
    "    for ele in [w1, b1, w2, b2]:\n",
    "        packed_value += list(ele.flatten())\n",
    "\n",
    "    return packed_value\n",
    "\n",
    "packed_wb = pack_wb(w1, b1, w2 ,b2)\n",
    "\n",
    "def unpack_wb(packed_wb, shape_list):\n",
    "    unpacked_list = []\n",
    "    index = 0\n",
    "    old_index = 0\n",
    "    for shape in shape_list:\n",
    "        old_index = index\n",
    "        shape_true = shape[0] * shape[1] \n",
    "        index += shape_true\n",
    "        matrix = np.array(packed_wb[old_index : index]).reshape(shape)\n",
    "        unpacked_list.append(matrix)\n",
    "\n",
    "    return unpacked_list\n",
    "\n",
    "def func_forward_w2(packed_wb, x, y, shape_list):\n",
    "    \n",
    "    w1, b1, w2, b2 = unpack_wb(packed_wb, shape_list)\n",
    "    \n",
    "    z1 = x.dot(w1) + b1\n",
    "    h1 = relu(z1)\n",
    "    z2 = h1.dot(w2) + b2\n",
    "    y_hat = softmax(z2)\n",
    "    \n",
    "    return J(y, y_hat)\n",
    "\n",
    "def func_back_w2(packed_wb, x, y, shape_list):\n",
    "    \n",
    "    w1, b1, w2, b2 = unpack_wb(packed_wb, shape_list)\n",
    "    \n",
    "    # feed forward process, store info in each layer\n",
    "    z1 = x.dot(w1) + b1\n",
    "    h1 = relu(z1)\n",
    "    z2 = h1.dot(w2) + b2\n",
    "    y_hat = softmax(z2)\n",
    "\n",
    "    # gradient descent for w2\n",
    "    dj_dw2 = (h1.T).dot(y_hat - y)\n",
    "\n",
    "    # gradient descent for b2\n",
    "    dj_db2 = (y_hat - y)\n",
    "\n",
    "    # gradient descent for w1\n",
    "    dj_dh1 = (y_hat - y).dot((w2).T)\n",
    "    dj_dz1 = dj_dh1\n",
    "    dj_dz1[z1 < 0] = 0\n",
    "    dj_dw1 = (x.T).dot(dj_dz1)\n",
    "    \n",
    "    # gradient descent for b1\n",
    "    dj_db1 = dj_dz1\n",
    "    \n",
    "    packed_der = pack_wb(dj_dw1, dj_db1, dj_dw2 ,dj_db2)\n",
    "#     for ele in [dj_dw1, dj_db1, dj_dw2 ,dj_db2]:\n",
    "#         print ele.shape\n",
    "\n",
    "    return packed_der\n",
    "\n",
    "# print check_grad(func_forward_w2, func_back_w2, packed_wb, x, y, shape_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
